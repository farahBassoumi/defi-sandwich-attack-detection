{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farahBassoumi/defi-sandwich-attack-detection/blob/main/extracted_data_reduction_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d2a97e4-33c7-40b5-9115-0ccbe4ff0cda",
      "metadata": {
        "id": "4d2a97e4-33c7-40b5-9115-0ccbe4ff0cda"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c7a224-9822-48db-ad53-de48ae9fa430",
      "metadata": {
        "id": "e3c7a224-9822-48db-ad53-de48ae9fa430"
      },
      "outputs": [],
      "source": [
        "sybil_df=pd.read_csv('address_is_sybil.csv',sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9966fc-985d-4216-a7fa-5799c934a759",
      "metadata": {
        "id": "1a9966fc-985d-4216-a7fa-5799c934a759"
      },
      "outputs": [],
      "source": [
        "sybil_df['is_sybil'] = sybil_df['is_sybil'].astype(bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e76bc56a-9566-4836-9dd3-7c92f6c4cd67",
      "metadata": {
        "id": "e76bc56a-9566-4836-9dd3-7c92f6c4cd67",
        "outputId": "1cb0ccf1-06b9-4124-a50b-0814a18a0e0d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>address</th>\n",
              "      <th>is_sybil</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1ab1c070c7f1958dbfc5537340cd8056580c43fc</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3c8cbd613857965267bcd4bdec7b794dd53969a0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2d93d0c4753e9b5c04a0e5b66a033ac71501fe6d</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b10bd34199663ebfbf20d740959d773e34030b59</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1e3a64ce9b4a573674284ab590b4fc538746fa21</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220584</th>\n",
              "      <td>2286fe256579ee6c5074d3a4c6d0502094c1e47b</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220585</th>\n",
              "      <td>3a46674c99e7be4d2f0aa5e46800f1cdb0ae7c50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220586</th>\n",
              "      <td>6d6a09aeec793c6ef96af5705a7b6a1ca4890af8</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220587</th>\n",
              "      <td>6d762a25a43976d3a58eb3771ebe17e6faa33395</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220588</th>\n",
              "      <td>d5c9c1b0e6349148e62aa10a4eeb0095afd4d8c6</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220589 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         address  is_sybil\n",
              "0       1ab1c070c7f1958dbfc5537340cd8056580c43fc      True\n",
              "1       3c8cbd613857965267bcd4bdec7b794dd53969a0      True\n",
              "2       2d93d0c4753e9b5c04a0e5b66a033ac71501fe6d      True\n",
              "3       b10bd34199663ebfbf20d740959d773e34030b59      True\n",
              "4       1e3a64ce9b4a573674284ab590b4fc538746fa21      True\n",
              "...                                          ...       ...\n",
              "220584  2286fe256579ee6c5074d3a4c6d0502094c1e47b     False\n",
              "220585  3a46674c99e7be4d2f0aa5e46800f1cdb0ae7c50     False\n",
              "220586  6d6a09aeec793c6ef96af5705a7b6a1ca4890af8     False\n",
              "220587  6d762a25a43976d3a58eb3771ebe17e6faa33395     False\n",
              "220588  d5c9c1b0e6349148e62aa10a4eeb0095afd4d8c6     False\n",
              "\n",
              "[220589 rows x 2 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sybil_df.head(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "228ea3fb-e014-4b2a-8a28-88848f4c0298",
      "metadata": {
        "id": "228ea3fb-e014-4b2a-8a28-88848f4c0298"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== Step 0: Global Settings ==========\n",
        "tqdm.pandas()\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "def report_memory(df, label=\"\"):\n",
        "    mem = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    tqdm.write(f\"{label}Memory usage: {mem:.2f} MB\")\n",
        "\n",
        "# ========== Step 1: Load CSV File ==========\n",
        "def load_csv_with_progress(filepath=\"merged_13.csv\") -> pd.DataFrame:\n",
        "    tqdm.write(f\"========== Step 1: Loading CSV File ({filepath}) ==========\")\n",
        "    total_rows = sum(1 for _ in open(filepath)) - 1  # exclude header\n",
        "    chunks = []\n",
        "    with pd.read_csv(filepath, sep=',', chunksize=10_000) as reader:\n",
        "        for chunk in tqdm(reader, total=total_rows // 10_000 + 1, desc=f\"📥 Reading {filepath}\"):\n",
        "            chunks.append(chunk)\n",
        "    df = pd.concat(chunks, ignore_index=True)\n",
        "    tqdm.write(f\"✅ Loaded {len(df):,} rows from {filepath}\")\n",
        "    return df\n",
        "\n",
        "# ========== Step 2: Optimize DataFrame ==========\n",
        "def optimize_dataframe(df):\n",
        "    tqdm.write(\"========== Step 2: Optimizing DataFrame ==========\")\n",
        "\n",
        "    for col in tqdm(df.select_dtypes(include='number').columns, desc=\"🔧 Downcasting numeric types\"):\n",
        "        if 'float' in str(df[col].dtype):\n",
        "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
        "        else:\n",
        "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
        "\n",
        "    for col in tqdm(df.select_dtypes(include='object').columns, desc=\"🔧 Converting object columns to category\"):\n",
        "        try:\n",
        "            pd.to_datetime(df[col].dropna().iloc[0])\n",
        "            continue\n",
        "        except Exception:\n",
        "            pass\n",
        "        if df[col].nunique() / len(df) < 0.5:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    for dt_col in ['detecttime', 'detect_date']:\n",
        "        if dt_col in df.columns:\n",
        "            df[dt_col] = pd.to_datetime(df[dt_col], errors='coerce')\n",
        "\n",
        "    if 'input' in df.columns:\n",
        "        tqdm.write(\"🧹 Dropping 'input' column\")\n",
        "        df.drop(columns=['input'], inplace=True)\n",
        "\n",
        "    tqdm.write(\"🧹 Dropping duplicate rows\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# ========== Step 3: Enrich Confirmed Transactions ==========\n",
        "def enrich_confirmed_transactions(df):\n",
        "    tqdm.write(\"========== Step 3: Enriching Confirmed Transactions ==========\")\n",
        "\n",
        "    df['detecttime'] = pd.to_datetime(df['detecttime'], errors='coerce')\n",
        "    confirmed_df = df[df[\"status\"] == \"confirmed\"].copy()\n",
        "    confirmed_df[\"was_evicted\"] = 0\n",
        "    confirmed_df[\"drop_reason\"] = None\n",
        "    confirmed_df[\"was_rejected\"] = 0\n",
        "    confirmed_df[\"rejection_reason\"] = None\n",
        "\n",
        "    grouped = df.groupby(\"hash\")\n",
        "\n",
        "    def enrich(row):\n",
        "        tx_hash = row[\"hash\"]\n",
        "        if tx_hash not in grouped.groups:\n",
        "            return pd.Series({\n",
        "                \"was_evicted\": 0,\n",
        "                \"drop_reason\": None,\n",
        "                \"was_rejected\": 0,\n",
        "                \"rejection_reason\": None\n",
        "            })\n",
        "\n",
        "        related = grouped.get_group(tx_hash)\n",
        "\n",
        "        evicted = related[related[\"status\"] == \"evicted\"]\n",
        "        was_evicted = len(evicted)\n",
        "        drop_reason = evicted[\"dropreason\"].mode().iloc[0] if was_evicted > 0 and not evicted[\"dropreason\"].isna().all() else None\n",
        "\n",
        "        rejected = related[related[\"status\"] == \"rejected\"]\n",
        "        was_rejected = len(rejected)\n",
        "        rejection_reason = rejected[\"rejectionreason\"].mode().iloc[0] if was_rejected > 0 and not rejected[\"rejectionreason\"].isna().all() else None\n",
        "\n",
        "        return pd.Series({\n",
        "            \"was_evicted\": was_evicted,\n",
        "            \"drop_reason\": drop_reason,\n",
        "            \"was_rejected\": was_rejected,\n",
        "            \"rejection_reason\": rejection_reason\n",
        "        })\n",
        "\n",
        "    confirmed_df[[\"was_evicted\", \"drop_reason\", \"was_rejected\", \"rejection_reason\"]] = confirmed_df.progress_apply(enrich, axis=1)\n",
        "\n",
        "    pending_times = df[df['status'] == 'pending'][['hash', 'detecttime']].rename(columns={'detecttime': 'pending_time'})\n",
        "    confirmed_df = confirmed_df.merge(pending_times, on='hash', how='left')\n",
        "\n",
        "    confirmed_df['pending_time'] = pd.to_datetime(confirmed_df['pending_time'], errors='coerce')\n",
        "    confirmed_df['time_pending'] = (confirmed_df['detecttime'] - confirmed_df['pending_time']).dt.total_seconds()\n",
        "    confirmed_df.drop(columns=['pending_time'], inplace=True)\n",
        "\n",
        "    final_df = df[~df['status'].isin(['confirmed', 'evicted', 'rejected'])].copy()\n",
        "    final_df = pd.concat([final_df, confirmed_df], ignore_index=True)\n",
        "    final_df.sort_values(by='detecttime', inplace=True)\n",
        "    final_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# ========== Step 4: Deduplicate Confirmed Transactions ==========\n",
        "def deduplicate_confirmed_transactions(df):\n",
        "    tqdm.write(\"========== Step 4: Deduplicating Confirmed Transactions ==========\")\n",
        "    confirmed_df = df[df['status'] == 'confirmed'].copy()\n",
        "    confirmed_df['detecttime'] = pd.to_datetime(confirmed_df['detecttime'], errors='coerce')\n",
        "    confirmed_df = confirmed_df.sort_values(by='detecttime').drop_duplicates(subset='hash', keep='first')\n",
        "\n",
        "    df = df[df['status'] != 'confirmed']\n",
        "    df = pd.concat([df, confirmed_df], ignore_index=True)\n",
        "\n",
        "    tqdm.write(f\"✅ Confirmed transactions deduplicated.\")\n",
        "    return df\n",
        "\n",
        "# ========== Step 5: Add Sybil Label ==========\n",
        "def add_sybil_label(df, sybil_df):\n",
        "    tqdm.write(\"========== Step 5: Merging is_sybil Labels ==========\")\n",
        "    try:\n",
        "        df['fromaddress_clean'] = df['fromaddress'].str.lower().str.replace(\"^0x\", \"\", regex=True)\n",
        "        sybil_df['address_clean'] = sybil_df['address'].str.lower().str.replace(\"^0x\", \"\", regex=True)\n",
        "\n",
        "        df = df.merge(\n",
        "            sybil_df[['address_clean', 'is_sybil']],\n",
        "            how='left',\n",
        "            left_on='fromaddress_clean',\n",
        "            right_on='address_clean'\n",
        "        )\n",
        "\n",
        "        df.drop(columns=['fromaddress_clean', 'address_clean'], inplace=True)\n",
        "\n",
        "        original_len = len(df)\n",
        "        df = df[df['is_sybil'].notna()].reset_index(drop=True)\n",
        "        tqdm.write(f\"🧹 Dropped {original_len - len(df)} rows without sybil label.\")\n",
        "    except Exception as e:\n",
        "        tqdm.write(f\"⚠️ Failed to merge is_sybil: {e}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ========== Step 6: Save Final CSV ==========\n",
        "def save_csv(df, prefix=\"df_labeled\"):\n",
        "    tqdm.write(\"========== Step 6: Saving Output CSV ==========\")\n",
        "    df.to_csv(f\"{prefix}.csv\", index=False)\n",
        "    tqdm.write(f\"✅ Saved as {prefix}.csv\")\n",
        "\n",
        "# ========== Master Callable Function ==========\n",
        "def run_full_pipeline(filepath: str, sybil_file: str = None, is_training: bool = True):\n",
        "    tqdm.write(\"========== 🏁 Starting Full Pipeline ==========\")\n",
        "\n",
        "    df = load_csv_with_progress(filepath)\n",
        "    report_memory(df, \"Original \")\n",
        "\n",
        "    df = optimize_dataframe(df)\n",
        "    report_memory(df, \"After Optimization \")\n",
        "\n",
        "    df = enrich_confirmed_transactions(df)\n",
        "    report_memory(df, \"After Enrichment \")\n",
        "\n",
        "    df = deduplicate_confirmed_transactions(df)\n",
        "    report_memory(df, \"After Deduplication \")\n",
        "\n",
        "    df.drop(columns=['reorg', 'replace', 'dropreason', 'rejectionreason', 'network'], inplace=True, errors='ignore')\n",
        "\n",
        "    if is_training:\n",
        "        if not sybil_file:\n",
        "            raise ValueError(\"You must provide sybil_file for training.\")\n",
        "        sybil_df = pd.read_csv(sybil_file)\n",
        "        df = add_sybil_label(df, sybil_df)\n",
        "        save_csv(df, prefix=f\"{filepath.split('.')[0]}_labeled_sybil\")\n",
        "    else:\n",
        "        save_csv(df, prefix=f\"{filepath.split('.')[0]}_for_prediction\")\n",
        "\n",
        "    tqdm.write(\"✅ Full Pipeline Complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c6ed44a-e5ca-4d21-94d7-11446e2adc1d",
      "metadata": {
        "id": "2c6ed44a-e5ca-4d21-94d7-11446e2adc1d"
      },
      "outputs": [],
      "source": [
        "run_full_pipeline(\n",
        "    filepath=\"merged_13.csv\",#exemple_file_name\n",
        "    sybil_file=\"sybil_addresses.csv\",\n",
        "    is_training=True  # or False for prediction\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc5d98a0-3948-4d58-a98f-a7b21dc260d2",
      "metadata": {
        "id": "cc5d98a0-3948-4d58-a98f-a7b21dc260d2"
      },
      "outputs": [],
      "source": [
        "run_sybil_processing_pipeline(\"prediction_df_for_day.csv\", is_training=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}